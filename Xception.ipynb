{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import densenet\n",
    "\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import regularizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import model_from_json\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.applications.xception import Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "current_dir = os.getcwd()\n",
    "LESSON_HOME_DIR = current_dir\n",
    "DATA_HOME_DIR = current_dir+'/data/'\n",
    "train_data_dir = DATA_HOME_DIR+'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda2/envs/tensorflow/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2016)\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import math\n",
    "import pickle\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from keras.utils import np_utils\n",
    "from keras.models import model_from_json\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_validation_set(train, target, test_size):\n",
    "    random_state = 51\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=test_size, random_state=random_state)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_validation_set_with_hold_out(train, target, test_size):\n",
    "    random_state = 51\n",
    "    train, X_test, target, y_test = train_test_split(train, target, test_size=test_size, random_state=random_state)\n",
    "    X_train, X_holdout, y_train, y_holdout = train_test_split(train, target, test_size=test_size, random_state=random_state)\n",
    "    return X_train, X_test, X_holdout, y_train, y_test, y_holdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another approach to loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import ImageFilter, ImageStat, Image, ImageDraw\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def im_multi(path):\n",
    "    try:\n",
    "        im_stats_im_ = Image.open(path)\n",
    "        return [path, {'size': im_stats_im_.size}]\n",
    "    except:\n",
    "        print(path)\n",
    "        return [path, {'size': [0,0]}]\n",
    "\n",
    "def im_stats(im_stats_df):\n",
    "    im_stats_d = {}\n",
    "    p = Pool(cpu_count())\n",
    "    ret = p.map(im_multi, im_stats_df['path'])\n",
    "    for i in range(len(ret)):\n",
    "        im_stats_d[ret[i][0]] = ret[i][1]\n",
    "    im_stats_df['size'] = im_stats_df['path'].map(lambda x: ' '.join(str(s) for s in im_stats_d[x]['size']))\n",
    "    return im_stats_df\n",
    "\n",
    "def get_im_cv2(path):\n",
    "    img = cv2.imread(path)\n",
    "    resized = cv2.resize(img, (32, 32), cv2.INTER_LINEAR) #use cv2.resize(img, (64, 64), cv2.INTER_LINEAR)\n",
    "    return [path, resized]\n",
    "\n",
    "def normalize_image_features(paths):\n",
    "    imf_d = {}\n",
    "    p = Pool(cpu_count())\n",
    "    ret = p.map(get_im_cv2, paths)\n",
    "    for i in range(len(ret)):\n",
    "        imf_d[ret[i][0]] = ret[i][1]\n",
    "    ret = []\n",
    "    fdata = [imf_d[f] for f in paths]\n",
    "    fdata = np.array(fdata, dtype=np.uint8)\n",
    "    fdata = fdata.transpose((0, 3, 1, 2))\n",
    "    fdata = fdata.astype('float32')\n",
    "    fdata = fdata / 255\n",
    "    return fdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = glob.glob('data/train/**/*.jpg')# + glob.glob('../input/additional/**/*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/train/Type_3/1127.jpg'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data', 'train', 'Type_3', '1127.jpg']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[1].split('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.DataFrame([[p.split('/')[2],p.split('/')[3],p] for p in train], columns = ['type','image','path'])#[::5] #limit for Kaggle Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = im_stats(train)\n",
    "train = train[train['size'] != '0 0'].reset_index(drop=True) #remove bad images\n",
    "train_data = normalize_image_features(train['path'])\n",
    "np.save('train.npy', train_data, allow_pickle=True, fix_imports=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "train_target = le.fit_transform(train['type'].values)\n",
    "train_target = np_utils.to_categorical(train_target, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "print(train_target[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Type_1' 'Type_2' 'Type_3']\n"
     ]
    }
   ],
   "source": [
    "print(le.classes_) #in case not 1 to 3 order\n",
    "np.save('train_target.npy', train_target, allow_pickle=True, fix_imports=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (1480, 3, 32, 32) (32 channels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1480, 3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_channels_last = np.transpose(train_data, (0, 2, 3, 1))\n",
    "# see: https://stackoverflow.com/a/23944468/6941138"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1480, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_data_channels_last.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = split_validation_set(train_data_channels_last, train_target, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1332, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printTrainableLayers(model):\n",
    "    for i, layer in enumerate(model.layers):\n",
    "       if layer.trainable == True: \n",
    "           print(i, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printAllLayers(model):\n",
    "    for i, layer in enumerate(model.layers):\n",
    "       print(i, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = Xception(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu', kernel_initializer='glorot_uniform')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(256, activation='relu', kernel_initializer='glorot_uniform')(x)\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(3, activation='softmax', kernel_initializer='glorot_uniform')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'input_1')\n",
      "(1, 'block1_conv1')\n",
      "(2, 'block1_conv1_bn')\n",
      "(3, 'block1_conv1_act')\n",
      "(4, 'block1_conv2')\n",
      "(5, 'block1_conv2_bn')\n",
      "(6, 'block1_conv2_act')\n",
      "(7, 'block2_sepconv1')\n",
      "(8, 'block2_sepconv1_bn')\n",
      "(9, 'block2_sepconv2_act')\n",
      "(10, 'block2_sepconv2')\n",
      "(11, 'block2_sepconv2_bn')\n",
      "(12, 'conv2d_1')\n",
      "(13, 'block2_pool')\n",
      "(14, 'batch_normalization_1')\n",
      "(15, 'add_1')\n",
      "(16, 'block3_sepconv1_act')\n",
      "(17, 'block3_sepconv1')\n",
      "(18, 'block3_sepconv1_bn')\n",
      "(19, 'block3_sepconv2_act')\n",
      "(20, 'block3_sepconv2')\n",
      "(21, 'block3_sepconv2_bn')\n",
      "(22, 'conv2d_2')\n",
      "(23, 'block3_pool')\n",
      "(24, 'batch_normalization_2')\n",
      "(25, 'add_2')\n",
      "(26, 'block4_sepconv1_act')\n",
      "(27, 'block4_sepconv1')\n",
      "(28, 'block4_sepconv1_bn')\n",
      "(29, 'block4_sepconv2_act')\n",
      "(30, 'block4_sepconv2')\n",
      "(31, 'block4_sepconv2_bn')\n",
      "(32, 'conv2d_3')\n",
      "(33, 'block4_pool')\n",
      "(34, 'batch_normalization_3')\n",
      "(35, 'add_3')\n",
      "(36, 'block5_sepconv1_act')\n",
      "(37, 'block5_sepconv1')\n",
      "(38, 'block5_sepconv1_bn')\n",
      "(39, 'block5_sepconv2_act')\n",
      "(40, 'block5_sepconv2')\n",
      "(41, 'block5_sepconv2_bn')\n",
      "(42, 'block5_sepconv3_act')\n",
      "(43, 'block5_sepconv3')\n",
      "(44, 'block5_sepconv3_bn')\n",
      "(45, 'add_4')\n",
      "(46, 'block6_sepconv1_act')\n",
      "(47, 'block6_sepconv1')\n",
      "(48, 'block6_sepconv1_bn')\n",
      "(49, 'block6_sepconv2_act')\n",
      "(50, 'block6_sepconv2')\n",
      "(51, 'block6_sepconv2_bn')\n",
      "(52, 'block6_sepconv3_act')\n",
      "(53, 'block6_sepconv3')\n",
      "(54, 'block6_sepconv3_bn')\n",
      "(55, 'add_5')\n",
      "(56, 'block7_sepconv1_act')\n",
      "(57, 'block7_sepconv1')\n",
      "(58, 'block7_sepconv1_bn')\n",
      "(59, 'block7_sepconv2_act')\n",
      "(60, 'block7_sepconv2')\n",
      "(61, 'block7_sepconv2_bn')\n",
      "(62, 'block7_sepconv3_act')\n",
      "(63, 'block7_sepconv3')\n",
      "(64, 'block7_sepconv3_bn')\n",
      "(65, 'add_6')\n",
      "(66, 'block8_sepconv1_act')\n",
      "(67, 'block8_sepconv1')\n",
      "(68, 'block8_sepconv1_bn')\n",
      "(69, 'block8_sepconv2_act')\n",
      "(70, 'block8_sepconv2')\n",
      "(71, 'block8_sepconv2_bn')\n",
      "(72, 'block8_sepconv3_act')\n",
      "(73, 'block8_sepconv3')\n",
      "(74, 'block8_sepconv3_bn')\n",
      "(75, 'add_7')\n",
      "(76, 'block9_sepconv1_act')\n",
      "(77, 'block9_sepconv1')\n",
      "(78, 'block9_sepconv1_bn')\n",
      "(79, 'block9_sepconv2_act')\n",
      "(80, 'block9_sepconv2')\n",
      "(81, 'block9_sepconv2_bn')\n",
      "(82, 'block9_sepconv3_act')\n",
      "(83, 'block9_sepconv3')\n",
      "(84, 'block9_sepconv3_bn')\n",
      "(85, 'add_8')\n",
      "(86, 'block10_sepconv1_act')\n",
      "(87, 'block10_sepconv1')\n",
      "(88, 'block10_sepconv1_bn')\n",
      "(89, 'block10_sepconv2_act')\n",
      "(90, 'block10_sepconv2')\n",
      "(91, 'block10_sepconv2_bn')\n",
      "(92, 'block10_sepconv3_act')\n",
      "(93, 'block10_sepconv3')\n",
      "(94, 'block10_sepconv3_bn')\n",
      "(95, 'add_9')\n",
      "(96, 'block11_sepconv1_act')\n",
      "(97, 'block11_sepconv1')\n",
      "(98, 'block11_sepconv1_bn')\n",
      "(99, 'block11_sepconv2_act')\n",
      "(100, 'block11_sepconv2')\n",
      "(101, 'block11_sepconv2_bn')\n",
      "(102, 'block11_sepconv3_act')\n",
      "(103, 'block11_sepconv3')\n",
      "(104, 'block11_sepconv3_bn')\n",
      "(105, 'add_10')\n",
      "(106, 'block12_sepconv1_act')\n",
      "(107, 'block12_sepconv1')\n",
      "(108, 'block12_sepconv1_bn')\n",
      "(109, 'block12_sepconv2_act')\n",
      "(110, 'block12_sepconv2')\n",
      "(111, 'block12_sepconv2_bn')\n",
      "(112, 'block12_sepconv3_act')\n",
      "(113, 'block12_sepconv3')\n",
      "(114, 'block12_sepconv3_bn')\n",
      "(115, 'add_11')\n",
      "(116, 'block13_sepconv1_act')\n",
      "(117, 'block13_sepconv1')\n",
      "(118, 'block13_sepconv1_bn')\n",
      "(119, 'block13_sepconv2_act')\n",
      "(120, 'block13_sepconv2')\n",
      "(121, 'block13_sepconv2_bn')\n",
      "(122, 'conv2d_4')\n",
      "(123, 'block13_pool')\n",
      "(124, 'batch_normalization_4')\n",
      "(125, 'add_12')\n",
      "(126, 'block14_sepconv1')\n",
      "(127, 'block14_sepconv1_bn')\n",
      "(128, 'block14_sepconv1_act')\n",
      "(129, 'block14_sepconv2')\n",
      "(130, 'block14_sepconv2_bn')\n",
      "(131, 'block14_sepconv2_act')\n",
      "(132, 'global_average_pooling2d_1')\n",
      "(133, 'dense_1')\n",
      "(134, 'dropout_1')\n",
      "(135, 'dense_2')\n",
      "(136, 'dropout_2')\n",
      "(137, 'dense_3')\n"
     ]
    }
   ],
   "source": [
    "printAllLayers(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set the first 310 layers (up to the last conv block)\n",
    "# to non-trainable (weights will not be updated)\n",
    "for layer in model.layers[:132]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132, 'global_average_pooling2d_2')\n",
      "(133, 'dense_4')\n",
      "(134, 'dropout_3')\n",
      "(135, 'dense_5')\n",
      "(136, 'dropout_4')\n",
      "(137, 'dense_6')\n"
     ]
    }
   ],
   "source": [
    "printTrainableLayers(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, None, None, 3) 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)            (None, None, None, 32 864         input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv1_bn (BatchNormalizat (None, None, None, 32 128         block1_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv1_act (Activation)    (None, None, None, 32 0           block1_conv1_bn[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)            (None, None, None, 64 18432       block1_conv1_act[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv2_bn (BatchNormalizat (None, None, None, 64 256         block1_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv2_act (Activation)    (None, None, None, 64 0           block1_conv2_bn[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "block2_sepconv1 (SeparableConv2D (None, None, None, 12 8768        block1_conv2_act[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "block2_sepconv1_bn (BatchNormali (None, None, None, 12 512         block2_sepconv1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "block2_sepconv2_act (Activation) (None, None, None, 12 0           block2_sepconv1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "block2_sepconv2 (SeparableConv2D (None, None, None, 12 17536       block2_sepconv2_act[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "block2_sepconv2_bn (BatchNormali (None, None, None, 12 512         block2_sepconv2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)                (None, None, None, 12 8192        block1_conv2_act[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)       (None, None, None, 12 0           block2_sepconv2_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNorm (None, None, None, 12 512         conv2d_5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "add_13 (Add)                     (None, None, None, 12 0           block2_pool[0][0]                \n",
      "                                                                   batch_normalization_5[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "block3_sepconv1_act (Activation) (None, None, None, 12 0           add_13[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "block3_sepconv1 (SeparableConv2D (None, None, None, 25 33920       block3_sepconv1_act[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "block3_sepconv1_bn (BatchNormali (None, None, None, 25 1024        block3_sepconv1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "block3_sepconv2_act (Activation) (None, None, None, 25 0           block3_sepconv1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "block3_sepconv2 (SeparableConv2D (None, None, None, 25 67840       block3_sepconv2_act[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "block3_sepconv2_bn (BatchNormali (None, None, None, 25 1024        block3_sepconv2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)                (None, None, None, 25 32768       add_13[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)       (None, None, None, 25 0           block3_sepconv2_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNorm (None, None, None, 25 1024        conv2d_6[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "add_14 (Add)                     (None, None, None, 25 0           block3_pool[0][0]                \n",
      "                                                                   batch_normalization_6[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "block4_sepconv1_act (Activation) (None, None, None, 25 0           add_14[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "block4_sepconv1 (SeparableConv2D (None, None, None, 72 188672      block4_sepconv1_act[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "block4_sepconv1_bn (BatchNormali (None, None, None, 72 2912        block4_sepconv1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "block4_sepconv2_act (Activation) (None, None, None, 72 0           block4_sepconv1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "block4_sepconv2 (SeparableConv2D (None, None, None, 72 536536      block4_sepconv2_act[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "block4_sepconv2_bn (BatchNormali (None, None, None, 72 2912        block4_sepconv2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)                (None, None, None, 72 186368      add_14[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)       (None, None, None, 72 0           block4_sepconv2_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNorm (None, None, None, 72 2912        conv2d_7[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "add_15 (Add)                     (None, None, None, 72 0           block4_pool[0][0]                \n",
      "                                                                   batch_normalization_7[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "block5_sepconv1_act (Activation) (None, None, None, 72 0           add_15[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "block5_sepconv1 (SeparableConv2D (None, None, None, 72 536536      block5_sepconv1_act[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "block5_sepconv1_bn (BatchNormali (None, None, None, 72 2912        block5_sepconv1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "block5_sepconv2_act (Activation) (None, None, None, 72 0           block5_sepconv1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "block5_sepconv2 (SeparableConv2D (None, None, None, 72 536536      block5_sepconv2_act[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "block5_sepconv2_bn (BatchNormali (None, None, None, 72 2912        block5_sepconv2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "block5_sepconv3_act (Activation) (None, None, None, 72 0           block5_sepconv2_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "block5_sepconv3 (SeparableConv2D (None, None, None, 72 536536      block5_sepconv3_act[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "block5_sepconv3_bn (BatchNormali (None, None, None, 72 2912        block5_sepconv3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "add_16 (Add)                     (None, None, None, 72 0           block5_sepconv3_bn[0][0]         \n",
      "                                                                   add_15[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "block6_sepconv1_act (Activation) (None, None, None, 72 0           add_16[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "block6_sepconv1 (SeparableConv2D (None, None, None, 72 536536      block6_sepconv1_act[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "block6_sepconv1_bn (BatchNormali (None, None, None, 72 2912        block6_sepconv1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "block6_sepconv2_act (Activation) (None, None, None, 72 0           block6_sepconv1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "block6_sepconv2 (SeparableConv2D (None, None, None, 72 536536      block6_sepconv2_act[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "block6_sepconv2_bn (BatchNormali (None, None, None, 72 2912        block6_sepconv2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "block6_sepconv3_act (Activation) (None, None, None, 72 0           block6_sepconv2_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "block6_sepconv3 (SeparableConv2D (None, None, None, 72 536536      block6_sepconv3_act[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "block6_sepconv3_bn (BatchNormali (None, None, None, 72 2912        block6_sepconv3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "add_17 (Add)                     (None, None, None, 72 0           block6_sepconv3_bn[0][0]         \n",
      "                                                                   add_16[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "block7_sepconv1_act (Activation) (None, None, None, 72 0           add_17[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "block7_sepconv1 (SeparableConv2D (None, None, None, 72 536536      block7_sepconv1_act[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "block7_sepconv1_bn (BatchNormali (None, None, None, 72 2912        block7_sepconv1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "block7_sepconv2_act (Activation) (None, None, None, 72 0           block7_sepconv1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "block7_sepconv2 (SeparableConv2D (None, None, None, 72 536536      block7_sepconv2_act[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "block7_sepconv2_bn (BatchNormali (None, None, None, 72 2912        block7_sepconv2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "block7_sepconv3_act (Activation) (None, None, None, 72 0           block7_sepconv2_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "block7_sepconv3 (SeparableConv2D (None, None, None, 72 536536      block7_sepconv3_act[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "block7_sepconv3_bn (BatchNormali (None, None, None, 72 2912        block7_sepconv3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "add_18 (Add)                     (None, None, None, 72 0           block7_sepconv3_bn[0][0]         \n",
      "                                                                   add_17[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "block8_sepconv1_act (Activation) (None, None, None, 72 0           add_18[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "block8_sepconv1 (SeparableConv2D (None, None, None, 72 536536      block8_sepconv1_act[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "block8_sepconv1_bn (BatchNormali (None, None, None, 72 2912        block8_sepconv1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "block8_sepconv2_act (Activation) (None, None, None, 72 0           block8_sepconv1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "block8_sepconv2 (SeparableConv2D (None, None, None, 72 536536      block8_sepconv2_act[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "block8_sepconv2_bn (BatchNormali (None, None, None, 72 2912        block8_sepconv2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "block8_sepconv3_act (Activation) (None, None, None, 72 0           block8_sepconv2_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "block8_sepconv3 (SeparableConv2D (None, None, None, 72 536536      block8_sepconv3_act[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "block8_sepconv3_bn (BatchNormali (None, None, None, 72 2912        block8_sepconv3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "add_19 (Add)                     (None, None, None, 72 0           block8_sepconv3_bn[0][0]         \n",
      "                                                                   add_18[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "block9_sepconv1_act (Activation) (None, None, None, 72 0           add_19[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "block9_sepconv1 (SeparableConv2D (None, None, None, 72 536536      block9_sepconv1_act[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "block9_sepconv1_bn (BatchNormali (None, None, None, 72 2912        block9_sepconv1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "block9_sepconv2_act (Activation) (None, None, None, 72 0           block9_sepconv1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "block9_sepconv2 (SeparableConv2D (None, None, None, 72 536536      block9_sepconv2_act[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "block9_sepconv2_bn (BatchNormali (None, None, None, 72 2912        block9_sepconv2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "block9_sepconv3_act (Activation) (None, None, None, 72 0           block9_sepconv2_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "block9_sepconv3 (SeparableConv2D (None, None, None, 72 536536      block9_sepconv3_act[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "block9_sepconv3_bn (BatchNormali (None, None, None, 72 2912        block9_sepconv3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "add_20 (Add)                     (None, None, None, 72 0           block9_sepconv3_bn[0][0]         \n",
      "                                                                   add_19[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "block10_sepconv1_act (Activation (None, None, None, 72 0           add_20[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "block10_sepconv1 (SeparableConv2 (None, None, None, 72 536536      block10_sepconv1_act[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "block10_sepconv1_bn (BatchNormal (None, None, None, 72 2912        block10_sepconv1[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "block10_sepconv2_act (Activation (None, None, None, 72 0           block10_sepconv1_bn[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "block10_sepconv2 (SeparableConv2 (None, None, None, 72 536536      block10_sepconv2_act[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "block10_sepconv2_bn (BatchNormal (None, None, None, 72 2912        block10_sepconv2[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "block10_sepconv3_act (Activation (None, None, None, 72 0           block10_sepconv2_bn[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "block10_sepconv3 (SeparableConv2 (None, None, None, 72 536536      block10_sepconv3_act[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "block10_sepconv3_bn (BatchNormal (None, None, None, 72 2912        block10_sepconv3[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "add_21 (Add)                     (None, None, None, 72 0           block10_sepconv3_bn[0][0]        \n",
      "                                                                   add_20[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "block11_sepconv1_act (Activation (None, None, None, 72 0           add_21[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "block11_sepconv1 (SeparableConv2 (None, None, None, 72 536536      block11_sepconv1_act[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "block11_sepconv1_bn (BatchNormal (None, None, None, 72 2912        block11_sepconv1[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "block11_sepconv2_act (Activation (None, None, None, 72 0           block11_sepconv1_bn[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "block11_sepconv2 (SeparableConv2 (None, None, None, 72 536536      block11_sepconv2_act[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "block11_sepconv2_bn (BatchNormal (None, None, None, 72 2912        block11_sepconv2[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "block11_sepconv3_act (Activation (None, None, None, 72 0           block11_sepconv2_bn[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "block11_sepconv3 (SeparableConv2 (None, None, None, 72 536536      block11_sepconv3_act[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "block11_sepconv3_bn (BatchNormal (None, None, None, 72 2912        block11_sepconv3[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "add_22 (Add)                     (None, None, None, 72 0           block11_sepconv3_bn[0][0]        \n",
      "                                                                   add_21[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "block12_sepconv1_act (Activation (None, None, None, 72 0           add_22[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "block12_sepconv1 (SeparableConv2 (None, None, None, 72 536536      block12_sepconv1_act[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "block12_sepconv1_bn (BatchNormal (None, None, None, 72 2912        block12_sepconv1[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "block12_sepconv2_act (Activation (None, None, None, 72 0           block12_sepconv1_bn[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "block12_sepconv2 (SeparableConv2 (None, None, None, 72 536536      block12_sepconv2_act[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "block12_sepconv2_bn (BatchNormal (None, None, None, 72 2912        block12_sepconv2[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "block12_sepconv3_act (Activation (None, None, None, 72 0           block12_sepconv2_bn[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "block12_sepconv3 (SeparableConv2 (None, None, None, 72 536536      block12_sepconv3_act[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "block12_sepconv3_bn (BatchNormal (None, None, None, 72 2912        block12_sepconv3[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "add_23 (Add)                     (None, None, None, 72 0           block12_sepconv3_bn[0][0]        \n",
      "                                                                   add_22[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "block13_sepconv1_act (Activation (None, None, None, 72 0           add_23[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "block13_sepconv1 (SeparableConv2 (None, None, None, 72 536536      block13_sepconv1_act[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "block13_sepconv1_bn (BatchNormal (None, None, None, 72 2912        block13_sepconv1[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "block13_sepconv2_act (Activation (None, None, None, 72 0           block13_sepconv1_bn[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "block13_sepconv2 (SeparableConv2 (None, None, None, 10 752024      block13_sepconv2_act[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "block13_sepconv2_bn (BatchNormal (None, None, None, 10 4096        block13_sepconv2[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)                (None, None, None, 10 745472      add_23[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "block13_pool (MaxPooling2D)      (None, None, None, 10 0           block13_sepconv2_bn[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNorm (None, None, None, 10 4096        conv2d_8[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "add_24 (Add)                     (None, None, None, 10 0           block13_pool[0][0]               \n",
      "                                                                   batch_normalization_8[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "block14_sepconv1 (SeparableConv2 (None, None, None, 15 1582080     add_24[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "block14_sepconv1_bn (BatchNormal (None, None, None, 15 6144        block14_sepconv1[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "block14_sepconv1_act (Activation (None, None, None, 15 0           block14_sepconv1_bn[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "block14_sepconv2 (SeparableConv2 (None, None, None, 20 3159552     block14_sepconv1_act[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "block14_sepconv2_bn (BatchNormal (None, None, None, 20 8192        block14_sepconv2[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "block14_sepconv2_act (Activation (None, None, None, 20 0           block14_sepconv2_bn[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glob (None, 2048)          0           block14_sepconv2_act[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 1024)          2098176     global_average_pooling2d_2[0][0] \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 1024)          0           dense_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 256)           262400      dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 256)           0           dense_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 3)             771         dropout_4[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 23,222,827\n",
      "Trainable params: 2,361,347\n",
      "Non-trainable params: 20,861,480\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath=\"Xception-top-improvement_learning_rate_decy_0.01-{epoch:02d}-{val_loss:}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "tbCallBack = TensorBoard(log_dir='./graph_xcep_top', histogram_freq=0, write_graph=True, write_images=True)\n",
    "callbacks_list.append(tbCallBack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = RMSprop(lr=1e-2, rho=0.9, decay=0.01)\n",
    "#opt = SGD(lr=1e-2, momentum=0.9, decay=0.01, nesterov=True)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "nb_classes = 3\n",
    "nb_epoch = 120\n",
    "nb_train_samples = X_train.shape[0]\n",
    "nb_val_samples = X_val.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        fill_mode='nearest')\n",
    "datagen.fit(train_data_channels_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_generator = datagen.flow(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample hat shape: (32,32,32,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 2.6587 - categorical_accuracy: 0.4771Epoch 00000: val_loss improved from inf to 1.47230, saving model to Xception-top-improvement_learning_rate_decy_0.01-00-1.47230166358.hdf5\n",
      "83/83 [==============================] - 6s - loss: 2.6412 - categorical_accuracy: 0.4765 - val_loss: 1.4723 - val_categorical_accuracy: 0.3514\n",
      "Epoch 2/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.7193 - categorical_accuracy: 0.4853Epoch 00001: val_loss improved from 1.47230 to 0.99398, saving model to Xception-top-improvement_learning_rate_decy_0.01-01-0.993979115744.hdf5\n",
      "83/83 [==============================] - 3s - loss: 1.6998 - categorical_accuracy: 0.4905 - val_loss: 0.9940 - val_categorical_accuracy: 0.5541\n",
      "Epoch 3/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0979 - categorical_accuracy: 0.5079Epoch 00002: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.1015 - categorical_accuracy: 0.5047 - val_loss: 1.0022 - val_categorical_accuracy: 0.5541\n",
      "Epoch 4/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0550 - categorical_accuracy: 0.5130Epoch 00003: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0559 - categorical_accuracy: 0.5127 - val_loss: 0.9993 - val_categorical_accuracy: 0.5541\n",
      "Epoch 5/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0446 - categorical_accuracy: 0.5166Epoch 00004: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0428 - categorical_accuracy: 0.5185 - val_loss: 0.9984 - val_categorical_accuracy: 0.5541\n",
      "Epoch 6/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0318 - categorical_accuracy: 0.5183 ETA: 0s - loss: 1.0360 - categorical_accuEpoch 00005: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0323 - categorical_accuracy: 0.5186 - val_loss: 1.0065 - val_categorical_accuracy: 0.5541\n",
      "Epoch 7/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0437 - categorical_accuracy: 0.5207Epoch 00006: val_loss improved from 0.99398 to 0.99334, saving model to Xception-top-improvement_learning_rate_decy_0.01-06-0.993341072186.hdf5\n",
      "83/83 [==============================] - 3s - loss: 1.0468 - categorical_accuracy: 0.5194 - val_loss: 0.9933 - val_categorical_accuracy: 0.5541\n",
      "Epoch 8/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0231 - categorical_accuracy: 0.5202Epoch 00007: val_loss improved from 0.99334 to 0.98947, saving model to Xception-top-improvement_learning_rate_decy_0.01-07-0.989474592982.hdf5\n",
      "83/83 [==============================] - 3s - loss: 1.0250 - categorical_accuracy: 0.5175 - val_loss: 0.9895 - val_categorical_accuracy: 0.5541\n",
      "Epoch 9/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0095 - categorical_accuracy: 0.5340Epoch 00008: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0124 - categorical_accuracy: 0.5321 - val_loss: 0.9925 - val_categorical_accuracy: 0.5541\n",
      "Epoch 10/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0269 - categorical_accuracy: 0.5141Epoch 00009: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0263 - categorical_accuracy: 0.5153 - val_loss: 0.9918 - val_categorical_accuracy: 0.5541\n",
      "Epoch 11/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0284 - categorical_accuracy: 0.5161Epoch 00010: val_loss improved from 0.98947 to 0.98663, saving model to Xception-top-improvement_learning_rate_decy_0.01-10-0.986634248012.hdf5\n",
      "83/83 [==============================] - 3s - loss: 1.0277 - categorical_accuracy: 0.5173 - val_loss: 0.9866 - val_categorical_accuracy: 0.5541\n",
      "Epoch 12/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0155 - categorical_accuracy: 0.5197Epoch 00011: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0143 - categorical_accuracy: 0.5214 - val_loss: 0.9877 - val_categorical_accuracy: 0.5541\n",
      "Epoch 13/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0137 - categorical_accuracy: 0.5228Epoch 00012: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0148 - categorical_accuracy: 0.5219 - val_loss: 0.9963 - val_categorical_accuracy: 0.5541\n",
      "Epoch 14/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0163 - categorical_accuracy: 0.5161Epoch 00013: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0171 - categorical_accuracy: 0.5157 - val_loss: 0.9876 - val_categorical_accuracy: 0.5541\n",
      "Epoch 15/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0099 - categorical_accuracy: 0.5263Epoch 00014: val_loss improved from 0.98663 to 0.98535, saving model to Xception-top-improvement_learning_rate_decy_0.01-14-0.985346645922.hdf5\n",
      "83/83 [==============================] - 3s - loss: 1.0109 - categorical_accuracy: 0.5257 - val_loss: 0.9853 - val_categorical_accuracy: 0.5541\n",
      "Epoch 16/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0142 - categorical_accuracy: 0.5225Epoch 00015: val_loss improved from 0.98535 to 0.98243, saving model to Xception-top-improvement_learning_rate_decy_0.01-15-0.98243128609.hdf5\n",
      "83/83 [==============================] - 3s - loss: 1.0156 - categorical_accuracy: 0.5223 - val_loss: 0.9824 - val_categorical_accuracy: 0.5541\n",
      "Epoch 17/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0172 - categorical_accuracy: 0.5142Epoch 00016: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0177 - categorical_accuracy: 0.5131 - val_loss: 0.9863 - val_categorical_accuracy: 0.5541\n",
      "Epoch 18/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0041 - categorical_accuracy: 0.5359Epoch 00017: val_loss improved from 0.98243 to 0.98121, saving model to Xception-top-improvement_learning_rate_decy_0.01-17-0.981213746844.hdf5\n",
      "83/83 [==============================] - 3s - loss: 1.0057 - categorical_accuracy: 0.5343 - val_loss: 0.9812 - val_categorical_accuracy: 0.5541\n",
      "Epoch 19/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0121 - categorical_accuracy: 0.5238Epoch 00018: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0146 - categorical_accuracy: 0.5214 - val_loss: 0.9831 - val_categorical_accuracy: 0.5541\n",
      "Epoch 20/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0118 - categorical_accuracy: 0.5179Epoch 00019: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0109 - categorical_accuracy: 0.5194 - val_loss: 0.9947 - val_categorical_accuracy: 0.5541\n",
      "Epoch 21/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0077 - categorical_accuracy: 0.5218Epoch 00020: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0054 - categorical_accuracy: 0.5250 - val_loss: 0.9865 - val_categorical_accuracy: 0.5541\n",
      "Epoch 22/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0016 - categorical_accuracy: 0.5259Epoch 00021: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0006 - categorical_accuracy: 0.5268 - val_loss: 0.9919 - val_categorical_accuracy: 0.5541\n",
      "Epoch 23/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0162 - categorical_accuracy: 0.5191Epoch 00022: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0173 - categorical_accuracy: 0.5187 - val_loss: 0.9980 - val_categorical_accuracy: 0.5541\n",
      "Epoch 24/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 0.9971 - categorical_accuracy: 0.5275Epoch 00023: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 0.9993 - categorical_accuracy: 0.5264 - val_loss: 0.9964 - val_categorical_accuracy: 0.5541\n",
      "Epoch 25/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0044 - categorical_accuracy: 0.5274Epoch 00024: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0061 - categorical_accuracy: 0.5256 - val_loss: 0.9985 - val_categorical_accuracy: 0.5541\n",
      "Epoch 26/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/83 [============================>.] - ETA: 0s - loss: 1.0123 - categorical_accuracy: 0.5257 ETA: 1s -Epoch 00025: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0111 - categorical_accuracy: 0.5266 - val_loss: 0.9860 - val_categorical_accuracy: 0.5541\n",
      "Epoch 27/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0095 - categorical_accuracy: 0.5140Epoch 00026: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0099 - categorical_accuracy: 0.5132 - val_loss: 1.0034 - val_categorical_accuracy: 0.5541\n",
      "Epoch 28/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 0.9957 - categorical_accuracy: 0.5375Epoch 00027: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 0.9975 - categorical_accuracy: 0.5343 - val_loss: 0.9905 - val_categorical_accuracy: 0.5541\n",
      "Epoch 29/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0082 - categorical_accuracy: 0.5242Epoch 00028: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0077 - categorical_accuracy: 0.5244 - val_loss: 0.9879 - val_categorical_accuracy: 0.5541\n",
      "Epoch 30/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0068 - categorical_accuracy: 0.5143Epoch 00029: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0054 - categorical_accuracy: 0.5151 - val_loss: 0.9889 - val_categorical_accuracy: 0.5541\n",
      "Epoch 31/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0120 - categorical_accuracy: 0.5242Epoch 00030: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0144 - categorical_accuracy: 0.5214 - val_loss: 0.9960 - val_categorical_accuracy: 0.5541\n",
      "Epoch 32/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0049 - categorical_accuracy: 0.5202Epoch 00031: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0041 - categorical_accuracy: 0.5209 - val_loss: 1.0024 - val_categorical_accuracy: 0.5541\n",
      "Epoch 33/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0044 - categorical_accuracy: 0.5244Epoch 00032: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0055 - categorical_accuracy: 0.5238 - val_loss: 1.0194 - val_categorical_accuracy: 0.5541\n",
      "Epoch 34/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0043 - categorical_accuracy: 0.5231Epoch 00033: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0043 - categorical_accuracy: 0.5236 - val_loss: 0.9988 - val_categorical_accuracy: 0.5541\n",
      "Epoch 35/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 0.9994 - categorical_accuracy: 0.5250Epoch 00034: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0033 - categorical_accuracy: 0.5225 - val_loss: 1.0076 - val_categorical_accuracy: 0.5541\n",
      "Epoch 36/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0041 - categorical_accuracy: 0.5198Epoch 00035: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0050 - categorical_accuracy: 0.5193 - val_loss: 1.0044 - val_categorical_accuracy: 0.5541\n",
      "Epoch 37/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0048 - categorical_accuracy: 0.5214Epoch 00036: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0050 - categorical_accuracy: 0.5220 - val_loss: 0.9944 - val_categorical_accuracy: 0.5541\n",
      "Epoch 38/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0087 - categorical_accuracy: 0.5234Epoch 00037: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0088 - categorical_accuracy: 0.5217 - val_loss: 0.9937 - val_categorical_accuracy: 0.5541\n",
      "Epoch 39/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0042 - categorical_accuracy: 0.5289Epoch 00038: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0038 - categorical_accuracy: 0.5294 - val_loss: 1.0022 - val_categorical_accuracy: 0.5541\n",
      "Epoch 40/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0056 - categorical_accuracy: 0.5191Epoch 00039: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0068 - categorical_accuracy: 0.5187 - val_loss: 0.9964 - val_categorical_accuracy: 0.5541\n",
      "Epoch 41/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0052 - categorical_accuracy: 0.5230Epoch 00040: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0049 - categorical_accuracy: 0.5240 - val_loss: 0.9932 - val_categorical_accuracy: 0.5541\n",
      "Epoch 42/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0051 - categorical_accuracy: 0.5247Epoch 00041: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0075 - categorical_accuracy: 0.5224 - val_loss: 0.9859 - val_categorical_accuracy: 0.5541\n",
      "Epoch 43/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0058 - categorical_accuracy: 0.5214Epoch 00042: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0081 - categorical_accuracy: 0.5209 - val_loss: 0.9856 - val_categorical_accuracy: 0.5541\n",
      "Epoch 44/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0066 - categorical_accuracy: 0.5262Epoch 00043: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0079 - categorical_accuracy: 0.5241 - val_loss: 0.9941 - val_categorical_accuracy: 0.5541\n",
      "Epoch 45/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0072 - categorical_accuracy: 0.5293Epoch 00044: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0071 - categorical_accuracy: 0.5279 - val_loss: 0.9965 - val_categorical_accuracy: 0.5541\n",
      "Epoch 46/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0052 - categorical_accuracy: 0.5201Epoch 00045: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0035 - categorical_accuracy: 0.5207 - val_loss: 0.9925 - val_categorical_accuracy: 0.5541\n",
      "Epoch 47/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0088 - categorical_accuracy: 0.5188Epoch 00046: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0091 - categorical_accuracy: 0.5191 - val_loss: 0.9958 - val_categorical_accuracy: 0.5541\n",
      "Epoch 48/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 0.9991 - categorical_accuracy: 0.5329Epoch 00047: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0009 - categorical_accuracy: 0.5321 - val_loss: 0.9933 - val_categorical_accuracy: 0.5541\n",
      "Epoch 49/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0034 - categorical_accuracy: 0.5154Epoch 00048: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0017 - categorical_accuracy: 0.5176 - val_loss: 1.0047 - val_categorical_accuracy: 0.5541\n",
      "Epoch 50/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0075 - categorical_accuracy: 0.5292Epoch 00049: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0082 - categorical_accuracy: 0.5292 - val_loss: 0.9994 - val_categorical_accuracy: 0.5541\n",
      "Epoch 51/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0074 - categorical_accuracy: 0.5166Epoch 00050: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0072 - categorical_accuracy: 0.5166 - val_loss: 0.9980 - val_categorical_accuracy: 0.5541\n",
      "Epoch 52/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0031 - categorical_accuracy: 0.5252Epoch 00051: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0023 - categorical_accuracy: 0.5246 - val_loss: 0.9959 - val_categorical_accuracy: 0.5541\n",
      "Epoch 53/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0121 - categorical_accuracy: 0.5214Epoch 00052: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0100 - categorical_accuracy: 0.5239 - val_loss: 0.9905 - val_categorical_accuracy: 0.5541\n",
      "Epoch 54/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/83 [============================>.] - ETA: 0s - loss: 1.0083 - categorical_accuracy: 0.5172Epoch 00053: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0067 - categorical_accuracy: 0.5198 - val_loss: 1.0222 - val_categorical_accuracy: 0.5541\n",
      "Epoch 55/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0105 - categorical_accuracy: 0.5198Epoch 00054: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0085 - categorical_accuracy: 0.5227 - val_loss: 1.0058 - val_categorical_accuracy: 0.5541\n",
      "Epoch 56/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0042 - categorical_accuracy: 0.5283Epoch 00055: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0024 - categorical_accuracy: 0.5306 - val_loss: 0.9955 - val_categorical_accuracy: 0.5541\n",
      "Epoch 57/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0025 - categorical_accuracy: 0.5231Epoch 00056: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0006 - categorical_accuracy: 0.5244 - val_loss: 1.0104 - val_categorical_accuracy: 0.5541\n",
      "Epoch 58/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0156 - categorical_accuracy: 0.5256Epoch 00057: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0165 - categorical_accuracy: 0.5250 - val_loss: 0.9923 - val_categorical_accuracy: 0.5541\n",
      "Epoch 59/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0115 - categorical_accuracy: 0.5172Epoch 00058: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0088 - categorical_accuracy: 0.5217 - val_loss: 0.9921 - val_categorical_accuracy: 0.5541\n",
      "Epoch 60/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0090 - categorical_accuracy: 0.5211Epoch 00059: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0105 - categorical_accuracy: 0.5214 - val_loss: 0.9934 - val_categorical_accuracy: 0.5541\n",
      "Epoch 61/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0027 - categorical_accuracy: 0.5260Epoch 00060: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0014 - categorical_accuracy: 0.5276 - val_loss: 1.0059 - val_categorical_accuracy: 0.5541\n",
      "Epoch 62/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0115 - categorical_accuracy: 0.5218Epoch 00061: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0139 - categorical_accuracy: 0.5191 - val_loss: 0.9977 - val_categorical_accuracy: 0.5541\n",
      "Epoch 63/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0060 - categorical_accuracy: 0.5323Epoch 00062: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0109 - categorical_accuracy: 0.5297 - val_loss: 1.0078 - val_categorical_accuracy: 0.5541\n",
      "Epoch 64/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0005 - categorical_accuracy: 0.5252Epoch 00063: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0003 - categorical_accuracy: 0.5254 - val_loss: 1.0133 - val_categorical_accuracy: 0.5541\n",
      "Epoch 65/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0105 - categorical_accuracy: 0.5231Epoch 00064: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0074 - categorical_accuracy: 0.5260 - val_loss: 1.0078 - val_categorical_accuracy: 0.5541\n",
      "Epoch 66/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 0.9997 - categorical_accuracy: 0.5312 ETA: 0s - loss: 1.0138 - cateEpoch 00065: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0019 - categorical_accuracy: 0.5285 - val_loss: 0.9918 - val_categorical_accuracy: 0.5541\n",
      "Epoch 67/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0109 - categorical_accuracy: 0.5167Epoch 00066: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0100 - categorical_accuracy: 0.5179 - val_loss: 0.9925 - val_categorical_accuracy: 0.5541\n",
      "Epoch 68/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 0.9992 - categorical_accuracy: 0.5297Epoch 00067: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0017 - categorical_accuracy: 0.5290 - val_loss: 1.0186 - val_categorical_accuracy: 0.5541\n",
      "Epoch 69/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0086 - categorical_accuracy: 0.5218Epoch 00068: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0101 - categorical_accuracy: 0.5205 - val_loss: 1.0036 - val_categorical_accuracy: 0.5541\n",
      "Epoch 70/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 0.9995 - categorical_accuracy: 0.5272Epoch 00069: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 0.9986 - categorical_accuracy: 0.5276 - val_loss: 1.0145 - val_categorical_accuracy: 0.5541\n",
      "Epoch 71/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0073 - categorical_accuracy: 0.5210Epoch 00070: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0062 - categorical_accuracy: 0.5224 - val_loss: 1.0050 - val_categorical_accuracy: 0.5541\n",
      "Epoch 72/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 0.9955 - categorical_accuracy: 0.5280Epoch 00071: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 0.9944 - categorical_accuracy: 0.5281 - val_loss: 1.0037 - val_categorical_accuracy: 0.5541\n",
      "Epoch 73/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0026 - categorical_accuracy: 0.5242Epoch 00072: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0050 - categorical_accuracy: 0.5218 - val_loss: 1.0051 - val_categorical_accuracy: 0.5541\n",
      "Epoch 74/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0110 - categorical_accuracy: 0.5245Epoch 00073: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0105 - categorical_accuracy: 0.5240 - val_loss: 0.9891 - val_categorical_accuracy: 0.5541\n",
      "Epoch 75/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0083 - categorical_accuracy: 0.5193Epoch 00074: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0087 - categorical_accuracy: 0.5185 - val_loss: 0.9879 - val_categorical_accuracy: 0.5541\n",
      "Epoch 76/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 0.9997 - categorical_accuracy: 0.5279Epoch 00075: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0001 - categorical_accuracy: 0.5269 - val_loss: 0.9952 - val_categorical_accuracy: 0.5541\n",
      "Epoch 77/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 0.9966 - categorical_accuracy: 0.5254Epoch 00076: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 0.9947 - categorical_accuracy: 0.5274 - val_loss: 0.9826 - val_categorical_accuracy: 0.5541\n",
      "Epoch 78/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0066 - categorical_accuracy: 0.5220Epoch 00077: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0047 - categorical_accuracy: 0.5241 - val_loss: 0.9941 - val_categorical_accuracy: 0.5541\n",
      "Epoch 79/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0084 - categorical_accuracy: 0.5237Epoch 00078: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0076 - categorical_accuracy: 0.5254 - val_loss: 1.0002 - val_categorical_accuracy: 0.5541\n",
      "Epoch 80/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0072 - categorical_accuracy: 0.5201Epoch 00079: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0054 - categorical_accuracy: 0.5226 - val_loss: 1.0095 - val_categorical_accuracy: 0.5541\n",
      "Epoch 81/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0118 - categorical_accuracy: 0.5140Epoch 00080: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0108 - categorical_accuracy: 0.5152 - val_loss: 1.0141 - val_categorical_accuracy: 0.5541\n",
      "Epoch 82/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/83 [============================>.] - ETA: 0s - loss: 1.0037 - categorical_accuracy: 0.5255Epoch 00081: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0040 - categorical_accuracy: 0.5249 - val_loss: 0.9928 - val_categorical_accuracy: 0.5541\n",
      "Epoch 83/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0002 - categorical_accuracy: 0.5258Epoch 00082: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0013 - categorical_accuracy: 0.5243 - val_loss: 0.9897 - val_categorical_accuracy: 0.5541\n",
      "Epoch 84/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0056 - categorical_accuracy: 0.5256Epoch 00083: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0069 - categorical_accuracy: 0.5251 - val_loss: 0.9902 - val_categorical_accuracy: 0.5541\n",
      "Epoch 85/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 0.9994 - categorical_accuracy: 0.5278Epoch 00084: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 0.9992 - categorical_accuracy: 0.5275 - val_loss: 0.9947 - val_categorical_accuracy: 0.5541\n",
      "Epoch 86/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0127 - categorical_accuracy: 0.5181Epoch 00085: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0116 - categorical_accuracy: 0.5191 - val_loss: 0.9864 - val_categorical_accuracy: 0.5541\n",
      "Epoch 87/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 0.9999 - categorical_accuracy: 0.5210Epoch 00086: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 0.9991 - categorical_accuracy: 0.5220 - val_loss: 0.9949 - val_categorical_accuracy: 0.5541\n",
      "Epoch 88/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 0.9976 - categorical_accuracy: 0.5201Epoch 00087: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 0.9986 - categorical_accuracy: 0.5208 - val_loss: 0.9823 - val_categorical_accuracy: 0.5541\n",
      "Epoch 89/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0026 - categorical_accuracy: 0.5190Epoch 00088: val_loss improved from 0.98121 to 0.98052, saving model to Xception-top-improvement_learning_rate_decy_0.01-88-0.980523177095.hdf5\n",
      "83/83 [==============================] - 3s - loss: 1.0017 - categorical_accuracy: 0.5208 - val_loss: 0.9805 - val_categorical_accuracy: 0.5541\n",
      "Epoch 90/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0055 - categorical_accuracy: 0.5194Epoch 00089: val_loss improved from 0.98052 to 0.97693, saving model to Xception-top-improvement_learning_rate_decy_0.01-89-0.976933294051.hdf5\n",
      "83/83 [==============================] - 3s - loss: 1.0030 - categorical_accuracy: 0.5212 - val_loss: 0.9769 - val_categorical_accuracy: 0.5541\n",
      "Epoch 91/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 0.9976 - categorical_accuracy: 0.5254Epoch 00090: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 0.9980 - categorical_accuracy: 0.5251 - val_loss: 0.9832 - val_categorical_accuracy: 0.5541\n",
      "Epoch 92/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0014 - categorical_accuracy: 0.5224Epoch 00091: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0018 - categorical_accuracy: 0.5241 - val_loss: 0.9858 - val_categorical_accuracy: 0.5541\n",
      "Epoch 93/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0077 - categorical_accuracy: 0.5262Epoch 00092: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0074 - categorical_accuracy: 0.5267 - val_loss: 0.9960 - val_categorical_accuracy: 0.5541\n",
      "Epoch 94/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 0.9962 - categorical_accuracy: 0.5294Epoch 00093: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 0.9938 - categorical_accuracy: 0.5313 - val_loss: 1.0248 - val_categorical_accuracy: 0.5541\n",
      "Epoch 95/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0127 - categorical_accuracy: 0.5221Epoch 00094: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0114 - categorical_accuracy: 0.5238 - val_loss: 1.0054 - val_categorical_accuracy: 0.5541\n",
      "Epoch 96/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0082 - categorical_accuracy: 0.5289Epoch 00095: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0091 - categorical_accuracy: 0.5286 - val_loss: 0.9876 - val_categorical_accuracy: 0.5541\n",
      "Epoch 97/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0104 - categorical_accuracy: 0.5104Epoch 00096: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0089 - categorical_accuracy: 0.5132 - val_loss: 0.9864 - val_categorical_accuracy: 0.5541\n",
      "Epoch 98/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0086 - categorical_accuracy: 0.5259Epoch 00097: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0087 - categorical_accuracy: 0.5257 - val_loss: 1.0131 - val_categorical_accuracy: 0.5541\n",
      "Epoch 99/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0026 - categorical_accuracy: 0.5247Epoch 00098: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0021 - categorical_accuracy: 0.5252 - val_loss: 1.0035 - val_categorical_accuracy: 0.5541\n",
      "Epoch 100/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0020 - categorical_accuracy: 0.5225Epoch 00099: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0001 - categorical_accuracy: 0.5234 - val_loss: 0.9950 - val_categorical_accuracy: 0.5541\n",
      "Epoch 101/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0044 - categorical_accuracy: 0.5308Epoch 00100: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0028 - categorical_accuracy: 0.5323 - val_loss: 1.0067 - val_categorical_accuracy: 0.5541\n",
      "Epoch 102/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0106 - categorical_accuracy: 0.5140Epoch 00101: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0097 - categorical_accuracy: 0.5151 - val_loss: 1.0015 - val_categorical_accuracy: 0.5541\n",
      "Epoch 103/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0055 - categorical_accuracy: 0.5198Epoch 00102: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0045 - categorical_accuracy: 0.5197 - val_loss: 0.9827 - val_categorical_accuracy: 0.5541\n",
      "Epoch 104/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 0.9975 - categorical_accuracy: 0.5283Epoch 00103: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 0.9968 - categorical_accuracy: 0.5295 - val_loss: 0.9951 - val_categorical_accuracy: 0.5541\n",
      "Epoch 105/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0054 - categorical_accuracy: 0.5221Epoch 00104: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0077 - categorical_accuracy: 0.5193 - val_loss: 0.9960 - val_categorical_accuracy: 0.5541\n",
      "Epoch 106/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 0.9841 - categorical_accuracy: 0.5369Epoch 00105: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 0.9831 - categorical_accuracy: 0.5367 - val_loss: 1.0065 - val_categorical_accuracy: 0.5541\n",
      "Epoch 107/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0151 - categorical_accuracy: 0.5126Epoch 00106: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0126 - categorical_accuracy: 0.5142 - val_loss: 1.0219 - val_categorical_accuracy: 0.5541\n",
      "Epoch 108/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0007 - categorical_accuracy: 0.5256Epoch 00107: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0011 - categorical_accuracy: 0.5243 - val_loss: 1.0053 - val_categorical_accuracy: 0.5541\n",
      "Epoch 109/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 0.9989 - categorical_accuracy: 0.5245Epoch 00108: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 2s - loss: 0.9986 - categorical_accuracy: 0.5243 - val_loss: 0.9940 - val_categorical_accuracy: 0.5541\n",
      "Epoch 110/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0009 - categorical_accuracy: 0.5218Epoch 00109: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0012 - categorical_accuracy: 0.5213 - val_loss: 1.0026 - val_categorical_accuracy: 0.5541\n",
      "Epoch 111/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0067 - categorical_accuracy: 0.5333Epoch 00110: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0071 - categorical_accuracy: 0.5325 - val_loss: 1.0001 - val_categorical_accuracy: 0.5541\n",
      "Epoch 112/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0138 - categorical_accuracy: 0.5127 ETA: 0s - loss: 1.0121 - categorical_accuEpoch 00111: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0136 - categorical_accuracy: 0.5136 - val_loss: 0.9994 - val_categorical_accuracy: 0.5541\n",
      "Epoch 113/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 0.9960 - categorical_accuracy: 0.5265Epoch 00112: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 0.9999 - categorical_accuracy: 0.5236 - val_loss: 0.9856 - val_categorical_accuracy: 0.5541\n",
      "Epoch 114/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0106 - categorical_accuracy: 0.5269Epoch 00113: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0131 - categorical_accuracy: 0.5247 - val_loss: 1.0174 - val_categorical_accuracy: 0.5541\n",
      "Epoch 115/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0099 - categorical_accuracy: 0.5257Epoch 00114: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0070 - categorical_accuracy: 0.5292 - val_loss: 0.9894 - val_categorical_accuracy: 0.5541\n",
      "Epoch 116/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0001 - categorical_accuracy: 0.5238Epoch 00115: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0019 - categorical_accuracy: 0.5236 - val_loss: 0.9904 - val_categorical_accuracy: 0.5541\n",
      "Epoch 117/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0017 - categorical_accuracy: 0.5269Epoch 00116: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0020 - categorical_accuracy: 0.5270 - val_loss: 1.0104 - val_categorical_accuracy: 0.5541\n",
      "Epoch 118/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0119 - categorical_accuracy: 0.5159Epoch 00117: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0091 - categorical_accuracy: 0.5193 - val_loss: 1.0074 - val_categorical_accuracy: 0.5541\n",
      "Epoch 119/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0020 - categorical_accuracy: 0.5311Epoch 00118: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0018 - categorical_accuracy: 0.5315 - val_loss: 0.9820 - val_categorical_accuracy: 0.5541\n",
      "Epoch 120/120\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.0050 - categorical_accuracy: 0.5158Epoch 00119: val_loss did not improve\n",
      "83/83 [==============================] - 2s - loss: 1.0034 - categorical_accuracy: 0.5162 - val_loss: 0.9902 - val_categorical_accuracy: 0.5541\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=nb_train_samples // batch_size,\n",
    "        epochs=nb_epoch,\n",
    "        validation_data=(X_val, Y_val),\n",
    "        validation_steps=nb_val_samples // batch_size,\n",
    "        callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"xcep_top_only.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"xcep_top_only.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Submit to kaggle for testing purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = glob.glob('data/test/**/*.jpg')\n",
    "test = pd.DataFrame([[p.split('/')[3],p] for p in test], columns = ['image','path']) #[::20] #limit for Kaggle Demo\n",
    "test_data = normalize_image_features(test['path'])\n",
    "np.save('test.npy', test_data, allow_pickle=True, fix_imports=True)\n",
    "\n",
    "test_id = test.image.values\n",
    "np.save('test_id.npy', test_id, allow_pickle=True, fix_imports=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data_channels_last = np.transpose(test_data, (0, 2, 3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(test_data_channels_last.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "# Calculate class posteriors probabilities\n",
    "y_probabilities = model.predict(test_data_channels_last, batch_size=16, verbose=0)\n",
    "print(len(y_probabilities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.17143506  0.51594162  0.31262332]\n",
      " [ 0.17145009  0.51688492  0.31166497]\n",
      " [ 0.13735089  0.47895184  0.38369721]\n",
      " [ 0.1652036   0.50534153  0.32945487]\n",
      " [ 0.17166527  0.51636571  0.31196898]]\n"
     ]
    }
   ],
   "source": [
    "print(y_probabilities[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(y_probabilities, columns=['Type_1','Type_2','Type_3'])\n",
    "df['image_name'] = test_id\n",
    "df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mdata\u001b[0m/\r\n",
      "Data_Exploration.ipynb\r\n",
      "densenet.py\r\n",
      "densenet.pyc\r\n",
      "\u001b[01;34mgood_run\u001b[0m/\r\n",
      "\u001b[01;34mgraph_xcep_top\u001b[0m/\r\n",
      "InceptionV3.ipynb\r\n",
      "\u001b[01;34mmodels\u001b[0m/\r\n",
      "\u001b[01;34mresults\u001b[0m/\r\n",
      "Simple Starter.ipynb\r\n",
      "submission.csv\r\n",
      "test_id.npy\r\n",
      "test.npy\r\n",
      "train.npy\r\n",
      "train_target.npy\r\n",
      "Xception.ipynb\r\n",
      "Xception-top-improvement-00-7.84123565055.hdf5\r\n",
      "Xception-top-improvement_learning_rate_decy_0.01-00-1.47230166358.hdf5\r\n",
      "Xception-top-improvement_learning_rate_decy_0.01-01-0.993979115744.hdf5\r\n",
      "Xception-top-improvement_learning_rate_decy_0.01-06-0.993341072186.hdf5\r\n",
      "Xception-top-improvement_learning_rate_decy_0.01-07-0.989474592982.hdf5\r\n",
      "Xception-top-improvement_learning_rate_decy_0.01-10-0.986634248012.hdf5\r\n",
      "Xception-top-improvement_learning_rate_decy_0.01-14-0.985346645922.hdf5\r\n",
      "Xception-top-improvement_learning_rate_decy_0.01-15-0.98243128609.hdf5\r\n",
      "Xception-top-improvement_learning_rate_decy_0.01-17-0.981213746844.hdf5\r\n",
      "Xception-top-improvement_learning_rate_decy_0.01-88-0.980523177095.hdf5\r\n",
      "Xception-top-improvement_learning_rate_decy_0.01-89-0.976933294051.hdf5\r\n",
      "xcep_top_only.h5\r\n",
      "xcep_top_only.json\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Line magic function `%kg` not found.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
